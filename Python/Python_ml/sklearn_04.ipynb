{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1364b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import date,datetime,timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import json\n",
    "import re\n",
    "from glob import glob\n",
    "from IPython.display import Image\n",
    "\n",
    "import seaborn as sns\n",
    "import folium as g\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "# 한글 폰트 문제 해결\n",
    "rc('font', family='AppleGothic')\n",
    "# 차트 축 <- 음수 부호 지원\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d8195d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold , cross_val_score, cross_validate,GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer, precision_recall_curve\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1410b47",
   "metadata": {},
   "source": [
    "#### 정확도 : 실 데이터와 예측 데이터가 얼마나 같은지를 판단하는 자료\n",
    "\n",
    "- 문제점 : 이진분류의 경우 모델의 성능을 왜곡할수 있다\n",
    "- why?:\n",
    "- solution : f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a65cfc",
   "metadata": {},
   "source": [
    "#### 분류모델 성능평가를 위한 이해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f13065f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 4\n",
      "TN: 2\n",
      "FN: 2\n",
      "FP: 3\n"
     ]
    }
   ],
   "source": [
    "target = [1,0,1,1,1,0,1,0,0,0,1]\n",
    "predict = [1,1,1,1,0,0,0,0,1,1,1]\n",
    "\n",
    "tp = tn = fn = fp = 0\n",
    "for i in range(len(target)):\n",
    "    # TP\n",
    "    if target[i]==1 and predict[i]==1:\n",
    "        tp+=1\n",
    "    # TN\n",
    "    elif target[i]==0 and predict[i]==0:\n",
    "        tn+=1\n",
    "    # FN\n",
    "    elif target[i]==1 and predict[i]==0:\n",
    "        fn+=1\n",
    "    # FP\n",
    "    elif target[i]==0 and predict[i]==1:\n",
    "        fp+=1\n",
    "        \n",
    "print('TP:',tp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)\n",
    "print('FP:',fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b495190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d042238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31768d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(target,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353b1c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(target,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b6d229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846153"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진분류\n",
    "# 제품을 생상하는 제조공장에서 품질 테스트를 실시하여 불량품을 찾아내고 불량품을 공장으로 돌려보낸다\n",
    "\n",
    "# 품질테스스 결과가 양성 ->불량품을 예측 음성->정상제품으로 예측\n",
    "\n",
    "# TP : 불량품이라고 예측한것중 실제 불량품인것\n",
    "# TN : 정상이라고 에측한것중 실제 정상인것\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dfdc32",
   "metadata": {},
   "source": [
    "1. 정밀도(precision) : TP/(FP+TP)\n",
    "- positive로 예측한 값들 중 실제 값이 postitive인 비율\n",
    "- 스팸메일\n",
    "2.  재현율(Recall) : TP/(FN+TP)\n",
    "- 실제 값이 postivie인 값들 중에 예측과 실제값이 postive로 일치하는 비율\n",
    "- 의학(암진단) 금융(사기 판별)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860efa3",
   "metadata": {},
   "source": [
    "* 정밀도와 재현율 사이에도 trade-off 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e75ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv('./Data/titanic_train.csv')\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695abc2a",
   "metadata": {},
   "source": [
    "- 정확도의 문제 : 이진 분류의 경우 데이터의 구성에 따라 모델의 성능이 왜곡될수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00d6bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEstimator(BaseEstimator):\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,X):\n",
    "        pred = np.zeros(X.shape[0],1)\n",
    "        for idx in range(X.shape[0]):\n",
    "            if X['Sex'].iloc[idx]==1:\n",
    "                pred[idx] = 0\n",
    "            else: \n",
    "                pred[idx] = 1\n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "234bfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "# 레일블 인코딩 Cabin , Sex , Embarked\n",
    "def drop_features(frm):\n",
    "    frm.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "    return frm\n",
    "\n",
    "\n",
    "def pre_processing(frm):\n",
    "    frm['Age'].fillna(frm['Age'].mean(), inplace=True)\n",
    "    frm['Cabin'].fillna('N', inplace=True)\n",
    "    frm['Embarked'].fillna('N', inplace=True)\n",
    "    return frm\n",
    "\n",
    "\n",
    "def label_encoder(frm):\n",
    "    frm['Cabin'] = frm['Cabin'].str[:1]\n",
    "    features = ['Cabin', 'Sex', 'Embarked']\n",
    "    for feature in features:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(frm[feature])\n",
    "        frm[feature] = encoder.transform(frm[feature])\n",
    "    return  frm\n",
    "\n",
    "def transform_features(frm):\n",
    "    frm = drop_features(frm)\n",
    "    frm = pre_processing(frm)\n",
    "    frm = label_encoder(frm)\n",
    "    return frm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db8dcc26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "886    0\n",
      "887    1\n",
      "888    0\n",
      "889    1\n",
      "890    0\n",
      "Name: Survived, Length: 891, dtype: int64\n",
      "     PassengerId  Pclass                                               Name  \\\n",
      "0              1       3                            Braund, Mr. Owen Harris   \n",
      "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2              3       3                             Heikkinen, Miss. Laina   \n",
      "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4              5       3                           Allen, Mr. William Henry   \n",
      "..           ...     ...                                                ...   \n",
      "886          887       2                              Montvila, Rev. Juozas   \n",
      "887          888       1                       Graham, Miss. Margaret Edith   \n",
      "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890       1                              Behr, Mr. Karl Howell   \n",
      "890          891       3                                Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
      "0      male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
      "1    female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
      "2    female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3    female  35.0      1      0            113803  53.1000  C123        S  \n",
      "4      male  35.0      0      0            373450   8.0500   NaN        S  \n",
      "..      ...   ...    ...    ...               ...      ...   ...      ...  \n",
      "886    male  27.0      0      0            211536  13.0000   NaN        S  \n",
      "887  female  19.0      0      0            112053  30.0000   B42        S  \n",
      "888  female   NaN      1      2        W./C. 6607  23.4500   NaN        S  \n",
      "889    male  26.0      0      0            111369  30.0000  C148        C  \n",
      "890    male  32.0      0      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "titanic_label = titanic_df['Survived']\n",
    "titanic_features = titanic_df.drop('Survived',axis=1)\n",
    "\n",
    "print(titanic_label)\n",
    "print(titanic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb7c1283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex        Age  SibSp  Parch     Fare  Cabin  Embarked\n",
       "0         3    1  22.000000      1      0   7.2500      7         3\n",
       "1         1    0  38.000000      1      0  71.2833      2         0\n",
       "2         3    0  26.000000      0      0   7.9250      7         3\n",
       "3         1    0  35.000000      1      0  53.1000      2         3\n",
       "4         3    1  35.000000      0      0   8.0500      7         3\n",
       "..      ...  ...        ...    ...    ...      ...    ...       ...\n",
       "886       2    1  27.000000      0      0  13.0000      7         3\n",
       "887       1    0  19.000000      0      0  30.0000      1         3\n",
       "888       3    0  29.699118      1      2  23.4500      7         3\n",
       "889       1    1  26.000000      0      0  30.0000      2         0\n",
       "890       3    1  32.000000      0      0   7.7500      7         2\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_features = transform_features(titanic_features)\n",
    "subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0511852",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(subset_features,titanic_label,test_size=0.2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72c04662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 8), (179, 8), (712,), (179,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0f35bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame,\n",
       " pandas.core.frame.DataFrame,\n",
       " pandas.core.series.Series,\n",
       " pandas.core.series.Series)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(X_test), type(y_train), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8e61484",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '1' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/m9cgmw1d3qs7ppj4ltvx26pc0000gn/T/ipykernel_2483/1304493888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muser_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muser_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/m9cgmw1d3qs7ppj4ltvx26pc0000gn/T/ipykernel_2483/3437361456.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '1' as a data type"
     ]
    }
   ],
   "source": [
    "user_model = UserEstimator()\n",
    "user_model.fit(X_train,y_train)\n",
    "y_pred = user_model.predict(X_test)\n",
    "\n",
    "print('accuracy',accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b84ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_eval(target,predict):\n",
    "    print(confusion_matrix(target,predict))\n",
    "    print('accuracy:',accuracy_score(target,predict))\n",
    "    print('precision:',precision_score(target,predict))\n",
    "    print('recall:',recall_score(target,predict))\n",
    "    print('f1-score:',f1_score(target,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62c21eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90 14]\n",
      " [21 54]]\n",
      "accuracy: 0.8044692737430168\n",
      "precision: 0.7941176470588235\n",
      "recall: 0.72\n",
      "f1-score: 0.7552447552447551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train,y_train)\n",
    "y_predict = lr_model.predict(X_test)\n",
    "metrics_eval(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30cb9363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8044692737430168\n",
      "precision: 0.7941176470588235\n",
      "recall: 0.72\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',(90+54)/(90+14+21+54))\n",
    "print('precision:',54/(14+54))\n",
    "print('recall:',54/(21+54))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2e511ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 교차검증\n",
    "fold = KFold(n_splits=20)\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score),\n",
    "           'recall':make_scorer(recall_score),\n",
    "           'f1-score':make_scorer(f1_score)}\n",
    "\n",
    "\n",
    "result = cross_validate(lr_model,X_train,y_train,cv=fold,scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74f3a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7895634920634922\n",
      "precision: 0.7316997456877488\n",
      "recall: 0.716394863306628\n",
      "f1-score: 0.7063641038192401\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',result['test_accuracy'].mean())\n",
    "print('precision:',result['test_precision'].mean())\n",
    "print('recall:',result['test_recall'].mean())\n",
    "print('f1-score:',result['test_f1-score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197473e",
   "metadata": {},
   "source": [
    "#### 유방암 데이터 : 정확<재현율\n",
    "- 모든 양성인 환자들을 기준으로 양성이라고 예측한 비율이 높을수록 좋은모델이라고 판단할수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eac3e009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': '/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/datasets/data/breast_cancer.csv'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(type(cancer))\n",
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a55aee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df = pd.DataFrame(data=cancer.data,columns=cancer.feature_names)\n",
    "cancer_df['target'] = cancer.target\n",
    "\n",
    "cancer_df.head()\n",
    "cancer_df['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e04df040",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_label = cancer_df['target']\n",
    "cancer_features = cancer_df.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97bb833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  2]\n",
      " [ 1 64]]\n",
      "accuracy: 0.9736842105263158\n",
      "precision: 0.9696969696969697\n",
      "recall: 0.9846153846153847\n",
      "f1-score: 0.9770992366412214\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(cancer_features,cancer_label,test_size=0.2,random_state=100)\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train,y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "metrics_eval(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33d06632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46  3]\n",
      " [ 3 62]]\n",
      "accuracy: 0.9473684210526315\n",
      "precision: 0.9538461538461539\n",
      "recall: 0.9538461538461539\n",
      "f1-score: 0.9538461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iganghui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(cancer_features,cancer_label,test_size=0.2,random_state=100)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train,y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "metrics_eval(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c6d1b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.958399209486166\n",
      "precision: 0.953672325481536\n",
      "recall: 0.9819444444444445\n",
      "f1-score: 0.9660963995200031\n"
     ]
    }
   ],
   "source": [
    "# 교차검증 cross_valide 후 성능평가\n",
    "fold = KFold(n_splits=20)\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score),\n",
    "           'recall':make_scorer(recall_score),\n",
    "           'f1-score':make_scorer(f1_score)}\n",
    "\n",
    "result = cross_validate(rf_model,X_train,y_train,cv=fold,scoring=scoring)\n",
    "print('accuracy:',result['test_accuracy'].mean())\n",
    "print('precision:',result['test_precision'].mean())\n",
    "print('recall:',result['test_recall'].mean())\n",
    "print('f1-score:',result['test_f1-score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561875a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤포레스트\n",
    "# n_estimators : 생성할 tree의 갯수를 의미\n",
    "# max_featuers : 선택할 feature의 수 \n",
    "# max_depth : 최대 선택할 트리의 깊이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bf68130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.26502343, 0.39485612, 0.2765746 , 0.41359693, 0.2697603 ,\n",
       "        0.40398116, 0.282415  , 0.4247148 ]),\n",
       " 'std_fit_time': array([0.00903684, 0.00393099, 0.00321161, 0.00457788, 0.00335989,\n",
       "        0.00383892, 0.00309525, 0.00374639]),\n",
       " 'mean_score_time': array([0.0095253 , 0.01353163, 0.00958247, 0.01353704, 0.00956314,\n",
       "        0.01377612, 0.00961288, 0.01358869]),\n",
       " 'std_score_time': array([0.00027284, 0.00024595, 0.0002938 , 0.0002786 , 0.00017959,\n",
       "        0.0002607 , 0.00023738, 0.00033364]),\n",
       " 'param_max_depth': masked_array(data=[5, 5, 5, 5, 6, 6, 6, 6],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[10, 10, 11, 11, 10, 10, 11, 11],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[200, 300, 200, 300, 200, 300, 200, 300],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 5, 'max_features': 10, 'n_estimators': 200},\n",
       "  {'max_depth': 5, 'max_features': 10, 'n_estimators': 300},\n",
       "  {'max_depth': 5, 'max_features': 11, 'n_estimators': 200},\n",
       "  {'max_depth': 5, 'max_features': 11, 'n_estimators': 300},\n",
       "  {'max_depth': 6, 'max_features': 10, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'max_features': 10, 'n_estimators': 300},\n",
       "  {'max_depth': 6, 'max_features': 11, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'max_features': 11, 'n_estimators': 300}],\n",
       " 'split0_test_score': array([1.        , 0.93333333, 1.        , 0.93333333, 0.93333333,\n",
       "        0.93333333, 1.        , 0.93333333]),\n",
       " 'split1_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split2_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split3_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split4_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split5_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split6_test_score': array([0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333]),\n",
       " 'split7_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split8_test_score': array([1.        , 0.93333333, 0.93333333, 0.93333333, 1.        ,\n",
       "        1.        , 0.93333333, 1.        ]),\n",
       " 'split9_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split10_test_score': array([0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333]),\n",
       " 'split11_test_score': array([0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333]),\n",
       " 'split12_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split13_test_score': array([1.        , 0.92857143, 1.        , 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split14_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split15_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split16_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split17_test_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.92857143, 1.        , 1.        ]),\n",
       " 'split18_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split19_test_score': array([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'mean_test_score': array([0.98642857, 0.97619048, 0.98309524, 0.97619048, 0.97952381,\n",
       "        0.97595238, 0.97952381, 0.97952381]),\n",
       " 'std_test_score': array([0.02715852, 0.03247186, 0.02929539, 0.03247186, 0.03130205,\n",
       "        0.03280102, 0.03130205, 0.03130205]),\n",
       " 'rank_test_score': array([1, 6, 2, 6, 3, 8, 3, 3], dtype=int32)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSerachCV\n",
    "X_train, X_test,y_train,y_test = train_test_split(cancer_features,cancer_label,test_size=0.2,random_state=100)\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "param = {\n",
    "    'n_estimators':[200,300],\n",
    "    'max_features':[10,11],\n",
    "    'max_depth':[5,6]\n",
    "}\n",
    "\n",
    "grid_rf_model = GridSearchCV(rf_model,\n",
    "                              param_grid=param,\n",
    "                              cv=20,\n",
    "                              scoring = 'recall',\n",
    "                              refit=True)\n",
    "\n",
    "grid_rf_model.fit(X_train,y_train)\n",
    "grid_rf_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "621a6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 10, 'n_estimators': 200}\n",
      "0.9864285714285714\n"
     ]
    }
   ],
   "source": [
    "print(grid_rf_model.best_params_)\n",
    "print(grid_rf_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbaf4b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  2]\n",
      " [ 1 64]]\n",
      "accuracy: 0.9736842105263158\n",
      "precision: 0.9696969696969697\n",
      "recall: 0.9846153846153847\n",
      "f1-score: 0.9770992366412214\n"
     ]
    }
   ],
   "source": [
    "y_predict = grid_rf_model.best_estimator_.predict(X_test)\n",
    "metrics_eval(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a647ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7251b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb04a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5a962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e639d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca26486",
   "metadata": {},
   "source": [
    "#### precision_recall_curve (실제 값, 예측 확률 값) : 임계값 변화에 따른 평가 지표 값을 반환\n",
    "- 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a3dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a2ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b714204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75b2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
